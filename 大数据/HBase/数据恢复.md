#### Log Splitting

HBase的最初阶段是使用如下流程进行日志切分的，整个过程都由 HMaster 控制执行。

![image-20200502175102654](/Users/licheng/Documents/Typora/Picture/image-20200502175102654.png)

* 将日志文件重命名

  如果不重命名，使用原始的日志文件，那么有可能会继续写入，因为有假死的情况存在：RegionServer 并没有真正的宕机，比如长时间 FullGC，但是 Master 却觉得它宕机了，这种情况写入更新操作还会继续发送到该 RegionServer。

* 读取 HLOG

  启动一个读线程依次顺序读出每个 HLog 中所有 <HLogKey,WALEdit> 数据对，根据 HLogKey 所属的 Region 不同写入不同的内存 buffer 中，如上图 Buffer-Region1 内存存放 Region1 对应的所有日志数据，这样整个HLog 所有数据会被完整 group 到不同的 buffer 中。

* 每个 buffer 会对应启动一个写线程，负责将 buffer 中的数据写入 HDFS 中 ，再等 Region 重新分配到其他 RegionServer 之后按顺序回放对应 Region 的日志数据。

  > HDFS 路径为：/hbase/table_name/region/recoverd.edits/.tmp

这种日志切分可以完成最基本的任务，但是效率极差，所以开发了 Distributed Log Splitting 架构。

#### Distributed Log Splitting

Distributed Log Splitting 是 Log Splitting 的**分布式实现**，它借助 HMaster 和所有 RegionServer 的计算能力进行日志切分，其中 HMaster 作为协调者，RegionServer 作为实际的工作者。

![img](/Users/licheng/Documents/Typora/Picture/4755539-0277604688f9d7e4.png)

HMaster 作为 HLog 切分任务的协调者，主要工作流程如下：

1. Master 会将待切分日志路径发布到 Zookeeper 节点上（/hbase/splitWAL），每个日志作为一个任务，每个任务都会有对应状态，起始状态为 TASK_UNASSIGNED。
2. 所有 RegionServer 启动之后都注册在这个节点上等待新任务，一旦 Master 发布任务之后，RegionServer 就会抢占该任务。
3. 抢占任务实际上首先去查看任务状态，如果是 TASK_UNASSIGNED 状态，说明当前没有人占有，此时就去修改该节点状态为 TASK_OWNED。如果修改失败，说明其他 RegionServer 也在抢占，修改成功表明任务抢占成功。
4. RegionServer 抢占任务成功之后会分发给相应线程处理，如果处理成功，会将该任务对应 Zookeeper 节点状态修改为 TASK_DONE，一旦失败会修改为 TASK_ERR。
5. Master 会一直监听在该 Zookeeper 节点上，一旦发生状态修改就会得到通知。任务状态变更为 TASK_ERR 的话，Master 会重新发布该任务，而变更为 TASK_DONE 的话，Master 会将对应的节点删除。

Regionserver 作为实际工作的执行者，抢占任务以及抢占任务之后的工作流程：

1. RegionServer 抢占到 Hlog 日志之后会分别给每个 Hlog 分配一个 HLogSplitter 线程进行处理，HLogSplitter 负责对日志文件执行具体的切分，切分思路还是首先读出日志中每一个 <HLogKey, WALEdit> 数据对，根据HLogKey 所属 Region 写入不同的 Region Buffer，**写入 Region Buffer 之前会对 Entrys 进行过滤，通过比较 sequenceid，如果发现该 Entry 已经 flush 了，就跳过这个 Entry。**
2. 每个 Region Buffer 都会有一个对应的写线程，将 buffer 中的日志数据写入 HDFS 中，写入路径为hdfs:/hbase/data/ns/table/region1/seqenceidN.temp，其中 seqenceid 是一个日志中某个 region 对应的最大 sequenceid。
3. 针对某一 Region 回放日志只需要将该 Region 对应的所有文件按照 sequenceid 由小到大依次进行回放即可

这种 Distributed Log Splitting 方式可以很大程度上加快整个故障恢复的进程，正常故障恢复时间可以降低到分钟级别。然而，这种方式会产生很多日志小文件，产生的文件数将会是 M * N，其中 M 是待切分的总 Hlog 数量，N是一个宕机 RegionServer上的 Region 个数。

#### Distributed Log Replay

相比 Distributed Log Splitting 方案，流程上的改动主要有两点：先重新分配 Region，再切分回放 HLog，但它在分解完 HLog 为 Region-Buffer 之后**并没有去写入小文件，而是直接去执行回放**。这样设计可以大大减少小文件的读写 IO 消耗，解决 DLS 的切身痛点。