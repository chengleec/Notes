### 集群架构

<img src="/Users/licheng/Documents/Typora/Picture/image-20200501212608441.png" alt="image-20200501212608441" style="zoom:67%;" />

* Application：用户编写的 Spark 应用程序。

* Driver：主程序，该进程运行 Application 的 main()  方法并且创建 SparkContext。

* Cluster manager：集群资源管理器(例如，Standlone  Manager，Mesos，YARN)。

* Worker：执行计算任务的工作节点。

* Executor：位于工作节点上的应用进程，负责执行计算任务并且将输出数据保存到内存或者磁盘中。每个 Application 都有各自独立的一批 Executor。

* Job：每个 Application 触发一次 Action 就提交一个 Job (DAG)。

* Stage：每个 Job 根据 RDD 依赖关系被拆分为多个 Stage。从后往前推算，遇到 ShuffleDependency 就断开形成一个 Stage，遇到 NarrowDependency 就将其加入该 stage。每个 stage 里面 Task 的数目由该 stage 最后一个 RDD 中的 Partition 个数决定。

* Task：Spark 中最小的任务执行单元。

  > InputSplit 与 Task 是一一对应的关系。这些具体的 Task 每个都会被分配到集群上的某个节点的某个 Executor 去执行。
  >
  > * 每个节点可以起一个或多个 Executor。
  > * 每个 Executor 由若干 core 组成，每个 Executor 的每个 core 一次只能执行一个 Task。
  > * 每个 Task 执行的结果就是生成了目标 RDD 的一个 partiton。
  >
  > 注意: 这里的 core 是虚拟的 core 而不是机器的物理 CPU 核，可以理解为就是 Executor 的一个工作线程。
  > 而 Task 被执行的并发度 = Executor 数目 * 每个 Executor 核数。

### Spark 提交任务

```shell
spark-submit \
--class spark_demo.WordCount
--master yarn \
--deploy-mode client \
--executor-memory 2G \ # 执行内存
--num-executors 4 \ # executor 数量
--executor-cores 2 \
/path/spark_demo.jar
```

#### Spark Job 执行流程

![image-20200718172117952](/Users/licheng/Documents/Typora/Picture/image-20200718172117952.png)

* Client 首先初试化 SparkContent，创建 DAGScheduler 和 TaskScheduler。然后向 YARN 的 ResourceManager 申请启动 Application Master。

* ResourceManager 收到请求后，在集群中选择一个 NodeManager，为该 Application 分配第一个 Container，要求它在这个 Container 中启动应用程序的 Application Master。

  > 与 YARN-Cluster 区别的是在该 ApplicationMaster 不运行 SparkContext，只与 SparkContext 进行联系进行资源的分派。

* Client 中的 SparkContext 初始化完毕后，与 Application Master 建立通讯，ApplicationMaster 根据任务信息向 ResourceManager 申请资源 (Container)。

* 一旦 Application Master 申请到资源后，便与对应的 NodeManager 通信，要求它在获得的 Container 中启动 CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend 启动后会向 Client 中的SparkContext 注册并申请 Task。

* SparkContext 构建成 DAG 图，然后 DAGScheduler 根据宽依赖切分 Stage，并将每个 Stage 中的 Tasks 打包成 TaskSet 交给 TaskScheduler。

* TaskScheduler 接收到 Task 后将其提交给相应的 SchedulerBackend。

* SchedulerBackend 主要用于对接不同的资源管理系统 (Yarn 是 YarnClientSchedulerBackend)。维护 Executor 相关信息 (包括 Executor 的地址、通信端口、host、总核数，剩余核数)，手头上 Executor 有多少被注册使用了，有多少剩余，总共还有多少核是空的等等。

  最终是由 SchedulerBackend 的 `launchTask()` 方法将 Task 发送到对应的 Executor 中。
  
*  CoarseGrainedExecutorBackend 将 Task 包装成 TaskRunner，并从线程池中抽取出一个空闲线程运行 Task，并向 Driver 汇报运行的状态和进度，以让 Client 随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。

   > 每个 CoarseGrainedExecutorBackend 能并行运行 Task 的数量就取决于分配给它的 CPU 的核数。

* 如果 Result 比较大，会先把执行结果 Result 存放到本地的 “内存＋磁盘” 上，由 blockManager 来管理，只把存储位置信息 (indirectResult) 发送给 Driver，Driver 需要实际的 Result 的时候，会通过 HTTP 去 fetch。如果 Result 不大 (小于`spark.akka.frameSize = 10MB`)，那么直接发送给 Driver。

* Driver 收到 Task 的执行结果后首先告诉 TaskScheduler 这个 Task 已经执行完，然后去分析 Result。

  * 如果结果是 ShuffleMapTask 的 MapStatus，那么需要将 MapStatus (ShuffleMapTask 输出的文件的位置和大小信息) 存放到 mapOutputTrackerMaster 中的 mapStatuses 数据结构中，方便 Shuffle Read 阶段查询 Shuffle Write 输出文件的位置。
  * 如果是 ResultTask 的结果，那么可以使用 ResultHandler 对 Result 进行 driver 端的计算 (比如 count() 会对所有 ResultTask 的 Result 作 sum)。
  
  > ShuffleMapTask 和 ResultTask 生成的 Result 不一样。
  >
  > ShuffleMapTask 生成的是 MapStatus，MapStatus 包含两项内容：一是该 Task 所在的 BlockManager 的 BlockManagerId (实际是 executorId + host, port, nettyPort)，二是 Task 输出的每个 FileSegment 大小。
  >
  > ResultTask 生成的 Result 的是 func 在 Partition 上的执行结果。比如 count() 的 func 就是统计 Partition 中 records 的个数。
  
* 如果 Driver 收到的结果是该 Stage 中的最后一个 Task 的 Result，那么可以 Submit 下一个 Stage，如果该 Stage 已经是最后一个 Stage，那么告诉 DAGScheduler job 已经完成。

* 应用程序运行完成后，Client 的 SparkContext 向 ResourceManager 申请注销并关闭自己。

每个 Application 有专属的 Executor 进程，该进程在 Application 期间一直驻留，并以多线程方式运行 Tasks。这种 Application 隔离机制有其优势的，无论是从调度角度看 (每个 Driver 调度它自己的任务)，还是从运行角度看(来自不同 Application 的 Task 运行在不同的 JVM 中)。当然，这也意味着 Spark Application 不能跨应用程序共享数据，除非将数据写入到外部存储系统。

#### YARN-Client 与 YARN-Cluster 区别

* YARN-Cluster 模式下，Driver 运行在 AM (Application Master) 所在的节点中，当用户提交了作业之后，就可以关掉 Client，作业会继续在 YARN 上运行，因而 YARN-Cluster 模式不适合运行交互类型的作业；

* YARN-Client 模式下，Driver 运行在 Client 所在的节点上，Client 会和请求的 Container 通信来调度他们工作，也就是说 Client 不能离开。

### RDD 机制

RDD：弹性分布式数据集 (Resilient Distributed Datasets)，是 Spark 中最基本的抽象。可以基于 lineage 实现高效容错、任务失败重试等特性，并且一个 RDD 可以有多个分区，每个分区可以设定分区函数。

RDD 是不可变的，因为这样可以更好的通过 lineage 实现数据容错。如果 transformation 不是创建一个新的 RDD，而是对原来的 RDD 做修改的话，那么数据容错就很难实现。

RDD 的来源可以来自内存集合或者外部存储系统，也可以由其他 RDD 转换而来。

<img src="/Users/licheng/Documents/Typora/Picture/image-20200627204802486.png" alt="image-20200627204802486" style="zoom:50%;" />

#### 创建 RDD

* 由现有集合创建,可指定 Rdd 分区数量 

  ```scala
  val RDD = sc.parallelize(List(1,2,3,4,5,6,7,8),3) 
  ```

* 引用外部存储系统中的数据集

  ```scala
  val RDD = sc.textFile("/usr/file/emp.txt")
  ```

  textFile 和 wholeTextFiles两者都可以用来读取外部文件，但是返回格式是不同的：

  * textFile 返回格式是 RDD[String] ，返回的是就是文件内容，RDD 中每一个元素对应一行数据；
  * wholeTextFiles 返回格式是 RDD[(String, String)]，元组中第一个参数是文件路径，第二个参数是文件内容
  * 两者都提供第二个参数来控制最小分区数。

#### 操作 RDD

RDD 支持两种类型的操作：transformations (转换，从现有 RDD 创建新 RDD) 和 actions (在数据集上运行计算后将值返回到 driver 或写入到文件系统)。

RDD 中的所有 transformations 操作都是惰性的，它们只是记住这些transformations操作，但不会立即执行，只有遇到 action 操作后才会真正的进行计算，这类似于函数式编程中的惰性求值。

#### 缓存RDD

##### 缓存级别

Spark 速度非常快的一个原因是 RDD 支持缓存。成功缓存后，如果之后的操作使用到了该数据集，则直接从缓存中获取。虽然缓存也有丢失的风险，但是由于 RDD 之间的依赖关系，如果某个分区的缓存数据丢失，只需要重新计算该分区即可。

| 存储级别                                   | 含义                                                         |
| ------------------------------------------ | ------------------------------------------------------------ |
| MEMORY_ONLY                                | 默认的缓存级别，将 RDD 以反序列化  Java 对象的形式存储在 JVM 中。如果内存空间不够，则部分分区数据将不再缓存。 |
| MEMORY_AND_DISK                            | 将 RDD 以反序列化 Java  对象的形式存储 JVM 中。如果内存空间不够，将未缓存的分区数据存储到磁盘，在需要使用这些分区时从磁盘读取。 |
| MEMORY_ONLY_SER                            | 将 RDD 以序列化 Java  对象的形式进行存储（每个分区为一个 byte 数组）。这种方式比反序列化对象节省存储空间，但在读取时会增加 CPU 的计算负担。 |
| MEMORY_AND_DISK_SER                        | 类似于  MEMORY_ONLY_SER，但是溢出的分区数据会存储到磁盘，而不是在用到它们时重新计算。 |
| DISK_ONLY                                  | 只在磁盘上缓存 RDD                                           |
| MEMORY_ONLY_2,      MEMORY_AND_DISK_2, etc | 与上面的对应级别功能相同，但是会为每个分区在集群中建立2个副本。 |
| OFF_HEAP                                   | 与 MEMORY_ONLY_SER  类似，但将数据存储在堆外内存中。这需要启用堆外内存。 |

##### 使用缓存

缓存数据的方法有两个：persist 和 cache 。

persist 可以通过传递一个 StorageLevel 对象来设置缓存的存储级别。

cache 内部调用的也是 persist，它是 persist 的特殊化形式，等价于 persist(StorageLevel.MEMORY_ONLY)。

##### 移除缓存

Spark 会自动监视每个节点上的缓存使用情况，并按照最近最少使用 (LRU) 的规则删除旧数据分区。当然，你也可以使用 RDD.unpersist() 方法进行手动删除。

### 数据容错

#### Lineage 机制

相比其他系统的细颗粒度的内存数据更新级别的备份或者 LOG 机制，RDD 的 Lineage 记录的是粗颗粒度的特定数据 Transformation 操作 (如 filter、map、join等) 行为。当这个 RDD 的部分分区数据丢失时，它可以通过 Lineage 获取足够的信息来重新运算和恢复丢失的数据分区。

因为这种粗颗粒的数据模型，限制了 Spark 的运用场合，所以 Spark 并不适用于所有高性能要求的场景，但同时相比细颗粒度的数据模型，也带来了性能的提升。

##### 宽依赖和窄依赖

RDD 和它的父 RDD(s) 之间的依赖关系分为两种不同的类型：

* 窄依赖 (narrow dependency)：父 RDDs 的一个 Partition 最多被子 RDDs 一个 Partition 所依赖；
* 宽依赖 (wide dependency)：父 RDDs 的一个 Partition 可以被子 RDDs 的多个 Partition 所依赖。

窄依赖和宽依赖的概念主要用在两个地方：一个是容错中相当于 Redo 日志的功能；另一个是在调度中构建 DAG作为不同 Stage 的划分点。

##### 容错原理

窄依赖可以在某个计算节点上直接通过计算父 RDD 的某块数据计算得到子 RDD 对应的某块数据。

宽依赖则要等到父 RDD 所有数据都计算完成之后，并且父 RDD 的计算结果进行 hash 并传到对应节点上之后才能计算子 RDD。 所以在长“血统”链特别是有宽依赖的时候，需要在适当的时机设置数据检查点。

#### Checkpoint 机制

Checkpoint (本质是通过将 RDD 写入 Disk 做检查点) 是为了通过 Lineage 做容错的辅助，Lineage 过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的 RDD 开始重做 Lineage，就会减少开销。Checkpoint 适用场景：

* DAG 中的 Lineage 过长，重算开销太大。
* 在宽依赖上做 Checkpoint 获得的收益更大。

什么时候该启用 Checkpoint 呢？满足以下任一条件：

* 使用了 stateful 转换：如果 application 中使用了`updateStateByKey`或`reduceByKeyAndWindow`等 stateful 操作，必须提供 Checkpoint 目录来允许定时的 RDD Checkpoint。
* 希望能从意外中恢复 driver。

如果 streaming app 没有 stateful 操作，也允许 driver 挂掉后再次重启的进度丢失，就没有启用 checkpoint的必要了。

##### Checkpoint 流程

在 Job 触发 Action 的时候，会调用 `finalRDD.docheckpoint()` 方法，从后往前扫描，如果 RDD 需要做 Checkpoint，就将它标记，然后重新运行一次 Job 将 RDD 写入到 HDFS 中。每个分区的数据会先写入一个临时文件中， 如果全部输出完，将临时文件重命名；如果输出失败，就回滚删除临时文件。最后清除 RDD 的依赖信息，并设定该 RDD 的状态为 Checkpointed。

* Initialized： 首先需要使用 `rdd.checkpoint() `去设定哪些 RDD 需要 Checkpoint，设定后，该 RDD 就接受 RDDCheckpointData 管理。用户还要设定 Checkpoint 的存储路径，一般在 HDFS 上。

* marked for checkpointing：初始化后，RDDCheckpointData 会将 RDD 标记为 MarkedForCheckpoint。

* checkpointing in progress：每个 job 运行结束后会调用 `finalRdd.doCheckpoint()`，finalRdd 会顺着 computing chain 回溯扫描，碰到要 checkpoint 的 RDD 就将其标记为 CheckpointingInProgress，然后启动一个 job 来完成 checkpoint。

* checkpointed：Job 完成 Checkpoint 后，将该 RDD 的 Dependency 全部清掉，并设定该 RDD 状态为 Checkpointed。然后设置该 RDD 的 parentRDD 为 CheckpointRDD，该 CheckpointRDD 负责以后读取在文件系统上的 Checkpoint 文件，生成该 RDD 的 Partition。

##### Cache 与 Checkpoint 的区别

`rdd.persist()` 虽然可以将 RDD 持久化到磁盘，但这些数据由 blockManager 管理。一旦程序执行结束，blockManager 会 stop，被 Cache 到磁盘上的 RDD 也会被清空 (整个 blockManager 使用的 local 文件夹被删除)。

而 Checkpoint 将 RDD 持久化到 HDFS 或本地文件夹，如果不被手动 remove 掉，是一直存在的，也就是说可以被下一个 Driver program 使用，而 cached RDD 不能被其他 Dirver program 使用。

### SparkStreaming 优化

* 在使用 Join 的地方看是否可以使用广播变量在 Map 端 Join。
* 使用高效的算子：
  * 使用 ReduceByKey/aggregateByKey 来代替 groupByKey，因为前者可以进行局部聚合操作（Combine），减少网络 IO;
  * 使用 MapPartition 来代替 Map 操作， 尤其是在需要网络连接的地方（每个分区建立一条连接，而不是每条数据建立一条连接）；
* 根据资源情况，指定相应的 Executor 个数、Executor 内存和 Executor CPU 核数（决定了 Task 的并行度）。
* 调用 RDD.cache() 来缓存数据，这样也可以加快数据的处理，但是我们需要更多的内存资源。
* 使用 repartition 和 coalesce 提高并行度 （repartition 是 coalesce shuffle 为 true 时的实现）。

### Spark 与 MapReduce 区别

* Spark 的速度比 MapReduce 快，Spark 把运算的中间数据存放在内存，迭代计算效率更高；MapReduce 的中间结果需要保存到磁盘，比较影响性能。

* Spark 容错性高，它通过 RDD 的依赖关系来实现高效容错；MapReduce 容错可能只能重新计算了，成本较高。
* Spark 和 MapReduce 的使用场景：MapReduce 适用于离线大批量计算，而 Spark 则侧重内存以及实时计算。

#### Spark 中的 Task 与 MapReduce 中的 Task 区别

* 在 Yarn 的 NodeManager 节点上启动一个 MapTask 或者 ReduceTask，在物理上启动的是一个 jvm 进程。
* Spark 的 Task 是 Executor 进程中的一个线程。