#### 简介

Hive 是一个构建在 Hadoop 之上的数据仓库，它可以将结构化的数据文件映射成表，并提供类 SQL 查询功能，用于查询的 SQL 语句会被转化为 MapReduce 作业，然后提交到 Hadoop 上运行。
特点：

* 简单、容易上手 (提供了类似 sql 的查询语言 hql)，使得精通 sql 但是不了解 Java 编程的人也能很好地进行大数据分析；
* 灵活性高，可以自定义用户函数 (UDF) 和存储格式；
* 为超大的数据集设计的计算和存储能力，集群扩展容易;
* 统一的元数据管理，可与 presto／impala／sparksql 等共享数据；
* 执行延迟高，不适合做数据的实时处理，但适合做海量数据的离线处理。

#### Hive的体系架构

<img src="/Users/licheng/Documents/Typora/Picture/未命名图片.png" alt="未命名图片" style="zoom:50%;" />

可以用 command-line shell 和 thrift／jdbc 两种方式来操作数据：

* command-line shell：通过 hive 命令行的的方式来操作数据；
* thrift／jdbc：通过 thrift 协议按照标准的 JDBC 的方式操作数据。

在 Hive 中，表名、表结构、字段名、字段类型、表的分隔符等统一被称为元数据。所有的元数据默认存储在 Hive 内置的 derby 数据库中，但由于 derby 只能有一个实例，也就是说不能有多个命令行客户端同时访问，所以在实际生产环境中，通常使用 MySQL 代替 derby。

Hive 进行的是统一的元数据管理，就是说你在 Hive 上创建了一张表，然后在 presto／impala／sparksql 中都是可以直接使用的，它们会从 Metastore 中获取统一的元数据信息，同样的你在 presto／impala／sparksql 中创建一张表，在 Hive 中也可以直接使用。

#### 数据类型

* 基本数据类型

  | 大类                    | 具体类型                                                     |
  | ----------------------- | ------------------------------------------------------------ |
  | Integers                | TINYINT—1 字节的有符号整数       SMALLINT—2 字节的有符号整数<br>INT—4 字节的有符号整数      BIGINT—8 字节的有符号整数 |
  | Boolean                 | TRUE/FALSE                                                   |
  | Floating point  numbers | FLOAT— 单精度浮点型      DOUBLE—双精度浮点型                 |
  | Fixed point  numbers    | DECIMAL—用户自定义精度定点数，比如  DECIMAL(7,2)             |
  | String                  | STRING—指定字符集的字符序列      VARCHAR—具有最大长度限制的字符序列      CHAR—固定长度的字符序列 |
  | Date and time           | TIMESTAMP — 时间戳       DATE—日期类型<br>TIMESTAMP WITH LOCAL TIME ZONE — 时间戳，纳秒精度 |
  | Binary                  | BINARY—字节序列                                              |

  > TIMESTAMP 和 TIMESTAMP WITH LOCAL TIME ZONE 的区别如下：
  >
  > * TIMESTAMP WITH LOCAL TIME ZONE：用户提交时间给数据库时，会被转换成数据库所在的时区来保存。查询时则按照查询客户端的不同，转换为查询客户端所在时区的时间。
  > * TIMESTAMP ：提交什么时间就保存什么时间，查询时也不做任何转换。

* 复杂类型

  | 类型   | 描述                                                         | 示例                                    |
  | ------ | ------------------------------------------------------------ | --------------------------------------- |
  | STRUCT | 类似于对象，是字段的集合，字段的类型可以不同，可以使用  名称.字段名 方式进行访问 | STRUCT ('xiaoming',  12 , '2018-12-12') |
  | MAP    | 键值对的集合，可以使用名称[key]的方式访问对应的值            | map('a', 1, 'b', 2)                     |
  | ARRAY  | 数组是一组具有相同类型和名称的变量的集合，可以使用名称[index] 访问对应的值 | ARRAY('a', 'b', 'c',  'd')              |

#### 存储格式

Hive 会在 HDFS 为每个数据库上创建一个目录，数据库中的表是该目录的子目录，表中的数据会以文件的形式存储在对应的表目录下。

| 格式         | 说明                                                         |
| ------------ | ------------------------------------------------------------ |
| TextFile     | 存储为纯文本文件。 这是 Hive  默认的文件存储格式。这种存储方式数据不做压缩，磁盘开销大，数据解析开销大。 |
| SequenceFile | SequenceFile 是 Hadoop  API 提供的一种二进制文件，它将数据以<key,value>的形式序列化到文件中。这种二进制文件内部使用 Hadoop 的标准的  Writable 接口实现序列化和反序列化。它与 Hadoop API 中的 MapFile 是互相兼容的。Hive 中的 SequenceFile 继承自 Hadoop API 的 SequenceFile，不过它的 key 为空，使用 value 存放实际的值，这样是为了避免 MR 在运行 map  阶段进行额外的排序操作。 |
| RCFile       | RCFile 文件格式是 FaceBook  开源的一种 Hive 的文件存储格式，首先将表分为几个行组，对每个行组内的数据按列存储，每一列的数据都是分开存储。 |
| ORC Files    | ORC 是在一定程度上扩展了  RCFile，是对 RCFile 的优化。       |
| Avro Files   | Avro  是一个数据序列化系统，设计用于支持大批量数据交换的应用。它的主要特点有：支持二进制序列化方式，可以便捷，快速地处理大量数据；动态语言友好，Avro  提供的机制使动态语言可以方便地处理 Avro 数据。 |
| Parquet      | Parquet 是基于 Dremel  的数据模型和算法实现的，面向分析型业务的列式存储格式。它通过按列进行高效压缩和特殊的编码技术，从而在降低存储空间的同时提高了 IO 效率。 |

以上压缩格式中 ORC 和 Parquet 的综合性能突出，使用较为广泛，推荐使用这两种格式。

通常在创建表的时候使用 STORED AS 参数指定：

```sql
CREATE TABLE page_view(viewTime INT, userid BIGINT)
 ROW FORMAT DELIMITED
   FIELDS TERMINATED BY '\001'
   COLLECTION ITEMS TERMINATED BY '\002'
   MAP KEYS TERMINATED BY '\003'
 STORED AS SEQUENCEFILE;
```