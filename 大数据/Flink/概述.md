### Flink 集群架构

<img src="/Users/licheng/Documents/Typora/Picture/image-20200512144440559.png" alt="image-20200512144440559" style="zoom:67%;" />

#### JobManager

JobManager 的功能主要有：

* 将 JobGraph 转换成 ExecutionGraph，最终将 Execution Graph 拿来运行。
* Scheduler 组件负责 Task 的调度。
* Checkpoint Coordinator 组件负责协调整个任务的 Checkpoint，包括 Checkpoint 的开始和完成。
* 通过 Actor System 与 TaskManager 进行通信。
* 其它的一些功能，例如 Recovery Metadata，用于进行故障恢复时，可以从 Metadata 里面读取数据。

#### TaskManager

TaskManager 是负责具体任务的执行过程，在 JobManager 申请到资源之后开始启动。TaskManager 里面的主要组件有：

* Memory & I/O Manager，即内存 I/O 的管理。
* Network Manager，用来对网络方面进行管理。
* Actor system，用来负责网络的通信。

##### TaskSlot

TaskManager 被分成很多个 TaskSlot，每个任务都要运行在一个 TaskSlot 里面，每个 TaskSlot 代表了 TaskManager 的一个固定大小的资源子集，是资源调度的最小单位。(Slot 目前仅仅用来隔离 Task 的内存，不会涉及到 CPU 的隔离)

每个 TaskManager 有多个 Slot 的话，也就是说多个 Task 运行在同一个 JVM 中。而在同一个 JVM 进程中的 Task，可以共享 TCP 连接（基于多路复用）和心跳消息，可以减少数据的网络传输。也能共享一些数据结构，一定程度上减少了每个 Task 的消耗。

##### SlotSharingGroup

默认情况下，Flink 允许 subTasks 共享 Slot，条件是它们都来自同一个 Job 的不同 Task 的 subTask。结果可能一个 Slot 持有该 Job 的整个 pipeline。允许 Slot 共享有以下两点好处：

1. Flink 集群所需的 Task Slots 数与 Job 中最高的并行度一致。也就是说我们不需要再去计算一个程序总共会起多少个 Task 了。
2. 更容易获得更充分的资源利用。如果没有 Slot 共享，那么非密集型操作 source/flatmap 就会占用同密集型操作 keyAggregation/sink 一样多的资源。

#### Client

Client 负责接受用户的程序代码，将用户提交的 Flink 程序转换成一个 JobGraph，然后提交给 Job Manager 以便进一步执行。

### Flink On Yarn

#### Per-Job

Flink on Yarn 中的 Per Job 模式是指每次提交一个任务，然后任务运行完成之后资源就会被释放。适合执行时间较长的大作业。

##### 执行流程

<img src="/Users/licheng/Documents/Typora/Picture/image-20200810102316523.png" alt="image-20200810102316523" style="zoom:50%;" />

* 首先 Client 提交 Application，比如 JobGraph 或者 JARs。
* 接下来 Yarn 的 ResourceManager 会申请第一个 Container 启动 Application Master，然后运行 Flink ResourceManager 和 JobManager。
* 最后 Flink ResourceManager 向 Yarn ResourceManager 申请资源。当分配到资源后，启动 TaskManager。TaskManager 启动后向 Flink ResourceManager 进行注册，注册成功后 JobManager 就会分配具体的任务给 TaskManager 开始执行。

#### Session

每个 Job 会启动一个 JobManager，所有 Job 共享 Dispatcher 和 ResourceManager。适合规模小，执行时间短的作业。

##### 执行流程

<img src="/Users/licheng/Documents/Typora/Picture/image-20200810102725682.png" alt="image-20200810102725682" style="zoom:50%;" />

* 客户端提交 JobGraph 以及依赖 jar 包到 YarnResourceManager，接着 Yarn ResourceManager 分配第一个 Container 启动 Application Master。然后运行 Flink ResourceManager 以及 Dispatcher。
* Dispatcher 负责接收用户提供的作业，并且负责为这个新提交的作业拉起一个新的 JobManager 服务。
* JobManager 会根据 JobGraph 生成的 ExecutionGraph 向 Flink ResourceManager 申请 Slot，如果没有可用 Slot 就向 Yarn ResourceManager 申请 Container，Container 启动以后会注册到 Flink ResourceManager。
* 最后 JobManager 会将 subTask 部署到响应 Container 的 slot 中去执行。

#### Yarn 模式特点

* 资源的统一管理和调度。Yarn 集群中所有节点的资源（内存、CPU、磁盘、网络等）被抽象为 Container。计算框架需要资源进行运算任务时需要向 Resource Manager 申请 Container，YARN 按照特定的策略对资源进行调度和进行 Container 的分配。Yarn 模式能通过多种任务调度策略来利用提高集群资源利用率。例如 FIFO Scheduler、Capacity Scheduler、Fair Scheduler，并能设置任务优先级。
* 资源隔离：Yarn 使用了轻量级资源隔离机制 Cgroups 进行资源隔离以避免相互干扰，一旦 Container 使用的资源量超过事先定义的上限值，就将其杀死。
* 自动 failover 处理。例如 Yarn NodeManager 监控、Yarn ApplicationManager 异常恢复。

### Flink 数据倾斜

* 首先将 key 打散，将 key 转化为 key-随机数，保证数据散列。
* 对打散后的数据进行聚合统计，这时我们会得到数据比如：(key1-12,1)，(key1-13,19)，(key1-1,20)，(key2-7,11)，(key2-7,10)。
* 将散列 key 还原成我们之前传入的 key，即将加入的随机数删除。
* 二次 keyby 将结果再聚合，输出到下游。

### Flink 与 Spark Streaming

#### 相同点

* 基于内存计算。
* 有统一的批处理和流处理 API，都支持类似 SQL 的编程接口。
* 都有完善的错误恢复机制。
* 都支持 Exactly Once 的语义一致性。

#### 区别

* 延迟性：Spark 基于微批处理，把流数据看成一个个小的批处理数据块分别处理，所以延迟性只能做到秒级；Flink 基于事件处理，每当有新的数据输入都会立刻处理，是真正的流式处理，支持毫秒级计算。

* 时间机制：Spark Streaming 只支持处理时间机制；Flink 支持三种时间机制，事件时间，注入时间，处理时间，同时支持 watermark 机制处理滞后数据。

#### 适用场景

* 对实时性要求：SparkStreaming 基于 RDD，所以做的是一个微批处理，实时性只能做到秒级，Flink 是基于事件流，一次处理一条数据，所以可以达到毫秒级。
* 是否需要对事件时间处理：SparkStreaming 只支持处理时间，Flink 不仅支持处理时间，还支持事件时间，而且有 WaterMark 机制可以处理一个延时的数据。
* 对容错性的要求：Flink 的容错机制更好一点，他可以将每个 Task 的状态信息直接保存下来，而 SparkStreaming 需要通过 lineage 去重新计算数据。如果是窄依赖还好，直接计算父 RDD 的数据传过来就行了。但如果是宽依赖，需要等所有父 RDD 的数据都计算完成后，然后通过 Hash 分区将数据传给子 RDD。这个过程中可能存在一个重复计算，因为不是所有的子 RDD 都需要重新计算数据。