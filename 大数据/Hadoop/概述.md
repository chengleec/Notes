### HDFS架构

HDFS 采用的是 Master/Slave 架构，一个 HDFS 集群包含一个单独的 NameNode 和多个 DataNode 节点。

#### NameNode

NameNode 负责管理整个分布式系统的元数据，主要包括：

* 目录树结构；
* 文件到 Block 的映射关系；
* Block 副本及其存储位置等管理数据；
* DataNode 的状态监控，两者通过心跳机制来传递管理信息和数据信息，通过这种方式的信息传递，NameNode 可以获知每个 DataNode 保存的 Block 信息、DataNode 的健康状况、命令 DataNode 启动停止等。

这些数据保存在内存中，同时在磁盘保存两个元数据管理文件：fsimage 和 editlog。

* fsimage：是内存命名空间元数据在外存的镜像文件；
* editlog：则是各种元数据操作的 write-ahead-log 文件，在体现到内存数据变化前首先会将操作记入 editlog 中，以防止数据丢失。

#### Secondary NameNode

Secondary NameNode 并不是 NameNode 的热备机，而是定期从 NameNode 拉取 fsimage 和 editlog 文件，并对两个文件进行合并，形成新的 fsimage 文件并传回 NameNode，这样做的目的是减轻 NameNode 的工作压力，本质上 SNN 是一个提供检查点功能服务的服务点。

创建检查点的触发条件：

* 通常情况下，Secondary NameNode 每隔一个小时，创建检查点。参数设置：`fs.checkpoint.period`
* 当编辑日志大小达到 64 MB 创建检查点。系统每隔 5 分钟检查一次编辑日志大小。参数设置：`fs.checkpoint.size`

#### DataNode

负责数据块的实际存储和读写工作，Block 默认是128MB，当客户端上传一个大文件时，HDFS 会自动将其切割成固定大小的 Block，为了保证数据可用性，每个 Block 会以多备份的形式存储，默认是 3 份。

### HDFS 读写数据流程

#### HDFS 写数据流程

<img src="/Users/licheng/Documents/Typora/Picture/image-20200722110131802.png" alt="image-20200722110131802" style="zoom:67%;" />

* Client 调用 `FileSystem.create()` 方法，与 Namenode 进行 RPC 通信，NameNode 首先检查该路径的文件是否存在以及有没有权限创建该文件，假如 ok，就创建一个新文件，但是并不关联任何 Block，返回一个`FSDataOutputStream`对象。

* Client 调用`FSDataOutputStream.write()`方法，与 NameNode 进行通信，获取 DataNode 列表，将数据以 Packet 的形式写入 `FSDataOutputStream` 对象内部的数据队列中 (Packet 是一个 64kb 大小的数据包)，然后 DataStreamer 拉取数据队列中的数据并传输到第一个 DataNode 节点，并且将它放入一个确认队列中。

* 第一个 DataNode 节点写完后将其转发到第二个 DataNode 中，接着第二个 DataNode 节点写完后会将数据包转发到第三个 DataNode 节点。

* ResponseProcessor 线程接收从 Datanode 发送过来的 ack，如果是一个成功的 ack，表示所有 DataNode 都已经接收到这个 Packet，将这个 packet 从确认队列中删除。如果发生错误 (某个 DataNode 磁盘故障)：

  * 首先，Pipeline 数据流管道会被关闭，确认队列中的 packets 会被添加到数据队列的前面以确保不会发生 packets 数据包的丢失。
  * 接着，在正常 DataNode 节点上写入完成的 block 的 id 版本会升级，这样发生故障的 DataNode 节点上的 block 数据会在节点恢复正常后被删除。
  * 最后，重新创建一个新的 Pipeline，排除掉出错的那些 DataNode 节点，DataStreamer 线程继续从数据队列中发送 Packet。

  > 如果 Pipeline 中的多个节点在写数据时失败，那么只要写成功的 block 的数量达到`dfs.replication.min `(默认为 1)，那么就任务是写成功的，然后 NameNode 后通过异步的方式将 block 复制到其他节点，最后数据副本达到 `dfs.replication` 参数配置的个数。

* 当写入数据完成后，Client 调用 `close()`方法，关闭输出流。

* 调用`FileSystem.complete()`方法，告诉 NameNode 节点写入成功。

#### HDFS 读数据流程

<img src="/Users/licheng/Documents/Typora/Picture/image-20200702090302407.png" alt="image-20200702090302407" style="zoom:67%;" />

* Client 通过 `FileSystem.open()` 方法，去与 NameNode 进行 RPC 通信，返回该文件的 Block 列表，也就是返回`FSDataInputStream`对象。
* Client 调用 `FSDataInputStream.read()` 方法，读取距离最近的 DataNode 的数据，读取完毕后会做完整性检查，如果成功，关闭与当前 DataNode 的通信；如果失败，会去该 Block 所在的的第二个 DataNode 地址读取。
* 假如第一个 Block 读取完毕，文件还未结束，那么 FileSystem 会从 NameNode 获取下一批的 Block 的列表。
* Client 调用 `FSDataInputStream.close()` 方法，关闭输入流。

### Hadoop-Shell

| 命令                       | 说明           |
| -------------------------- | -------------- |
| hadoop fs -ls /            | 查看所有目录   |
| hadoop fs -ls -r /         | 递归显示文件夹 |
| hadoop fs -mkdir           | 创建目录       |
| hadoop fs -mkdir -p        | 递归创建文件夹 |
| hadoop fs -put Hello.txt / | 上传文件       |
| hadoop fs -get /Hello.txt  | 拷贝到本地     |
| hadoop fs -cat /Hello.txt  | 查看文件内容   |
| hadoop fs -rm /hello.txt   | 删除文件       |
| hadoop fs -rm -r /test     | 删除文件夹     |

