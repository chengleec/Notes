### Redis 主从复制架构

<img src="/Users/licheng/Documents/Typora/Picture/image-20200803150714549.png" alt="image-20200803150714549" style="zoom:67%;" />

Redis 的主从架构为一主多从， Master 负责写，并且将数据复制到其它的 Slave 节点，Slave 节点负责读。所有的读请求全部走 Slave 节点。这样也可以很轻松实现水平扩容，支撑读高并发。

#### 主从复制

<img src="/Users/licheng/Documents/Typora/Picture/image-20200803150947548.png" alt="image-20200803150947548" style="zoom: 67%;" />

##### 完整重同步

用于初次复制，通过让 Master 创建并发送 RDB 文件，以及向服务器发送保存在缓冲区的写命令来同步。

* Slave 节点向 Master 发送一个 `SYNC` 命令。

* 接到 `SYNC` 命令的 Master 将开始执行 `BGSAVE` ，并将所有新执行的写入命令都保存到一个缓冲区里面。

* 当 `BGSAVE` 执行完毕后，Master 将 RDB 文件发送给 Slave 节点，Slave 接收到这个 RDB 文件后先写入本地磁盘，然后再从本地磁盘加载到内存中。

  > 如果 RDB 复制时间超过 60秒 (repl-timeout)，那么 Slave 节点就会认为复制失败，可以适当调大这个参数。

* Master 将写命令缓冲区中积累的所有内容都发送给 Slave 节点。

##### 部分重同步

用于断线重连时，当 Slave 在断线后重新连接 Master 时，Master 可以将连接断开期间执行的写命令发送给 Slave 来同步数据。

* 当出现网络连接断开时， Slave 会重新连接， 并且向 Master 请求继续执行原来的复制进程。

* 如果 Slave 记录的 Master id 和当前连接的 Master 的 id 相同， 并且 Slave 记录的偏移量所指定的数据仍然保存在 Master 的缓冲区里面， 那么 Master 会向 Slave 发送断线时缺失的那部分数据， 然后复制工作可以继续执行。

* 否则的话， Slave 就要执行完整重同步操作。

> Master 会在内存中维护一个 backlog，Master 和 Slave 都会保存一个复制偏移量 (replica offset) 和一个 Master id (master run id)。

#### 主从复制的好处

* 读写分离：Master 写，Slave 读，提高服务器的读写负载能力。
* 负载均衡：基于主从结构，配合读写分离，由多个 Slave 分担 Master 负载，大大提高 Redis 服务器并发量与数据吞吐量。
* 故障恢复：当 Master 出现问题时，由 Slave 提供服务，实现快速的故障恢复。
* 数据冗余：实现数据热备份，是持久化之外的一种数据冗余方式。
* 高可用基石：基于主从复制，构建哨兵模式与集群，实现 Redis 的高可用方案。

### Redis 哨兵机制

#### Redis 哨兵集群架构图

<img src="/Users/licheng/Documents/Typora/Picture/image-20200803160543153.png" alt="image-20200803160543153" style="zoom: 50%;" />

#### 哨兵机制

哨兵机制是为了实现 Redis 集群高可用而引入的一种机制。哨兵的作用就是监控 Redis 系统的运行状况，在 Redis 中，哨兵叫做 sentinel。它的功能包括两个：

* 监控 Master 节点和 Slave 节点，是否正常运行。
* 当 Master 主节点发生故障时，自动将 Master 对应的 Slave 节点升级为 Master 节点，实现主从切换。

哨兵至少需要 3 个实例，来保证自己的健壮性。并且哨兵 + Redis 主从的部署架构，是**不保证数据零丢失**的，只能保证 Redis 集群的高可用。

导致数据丢失的两种情况

* 主备切换的过程，可能会导致数据丢失：因为 master -> slave 的复制是异步的，所以可能有部分数据还没复制到 slave，master 就宕机了，此时这部分数据就丢失了。
* 脑裂导致的数据丢失：脑裂时可能 client 还没来得及切换到新的 Master，还继续向旧 Master 写数据。因此旧 Master 再次恢复的时候，会被作为一个 Slave 挂到新的 Master 上去，自己的数据会清空，重新从新的 Master 复制数据。而新的 Master 并没有后来 client 写入的数据，因此，这部分数据也就丢失了。

#### 哨兵集群的自动发现

多个哨兵节点之间，会因为共同监听同一个 Master 节点或者 Salve 节点，从而产生关联。一个新加入的哨兵节点，会和监视相同 Master 节点的其他哨兵，通过发布/订阅机制来完成相互感知。最后新加入哨兵集群的哨兵，会和集群中的其他的哨兵建立起长连接，来共同维护 Redis 集群的高可用。

每个哨兵都会往 `__sentinel__:hello` 这个 channel 里发送一个消息，这时候所有其他哨兵都可以消费到这个消息，并感知到其他的哨兵的存在。

每隔两秒钟，每个哨兵都会往自己监控的某个 Master + Slaves 对应的 `__sentinel__:hello` channel 里发送一个消息，内容是自己的 host、ip 和 runid 还有对这个 master 的监控配置。

每个哨兵也会去监听自己监控的每个 Master + Slaves 对应的 `__sentinel__:hello` channel，然后去感知到同样在监听这个 Master + Slaves 的其他哨兵的存在。

每个哨兵还会跟其他哨兵交换对 Master 的监控配置，互相进行监控配置的同步。

#### Master 节点故障发现

Sentinel 哨兵节点会定时向 Master 节点发送心跳来判断 Master 节点是否存活。一旦 Master 节点在规定时间内没有正确响应，Sentinel 哨兵会把 Master 节点设置为"主观不可用状态"，然后它会把"主观不可用状态"发送给其他所有的集群中的其他 Sentinel 哨兵节点去确认，当哨兵集群中大多数哨兵节点确认不可以用时，便会认为该 Master 是“客观不可用”，接下来便会进入新的 Master 选举过程。

### Redis Cluster 集群模式

Redis 的哨兵模式基本已经可以实现高可用，读写分离 ，但是在这种模式下每台 Redis 服务器都存储相同的数据，很浪费内存，所以在 Redis 3.0 上加入了 Cluster 集群模式，实现了 Redis 的分布式存储，也就是说每台 Redis 节点上存储不同的内容。

Redis Cluster 基于一致性哈希算法实现分布式存储。Redis 集群有 16384 个哈希槽，每个 key 通过 CRC 校验后对 16384 取模来决定放置哪个槽。集群的每个节点负责一部分 hash 槽，举个例子，比如当前集群有 3 个节点，那么：

* 节点 A 包含 0 到 5460 号哈希槽
* 节点 B 包含 5461 到 10922 号哈希槽
* 节点 C 包含 10923 到 16383 号哈希槽

#### 一致性 hash 算法

一致性 Hash 算法是将所有的哈希值构成了一个环，其范围在 0 ~ 2^32^-1。如下图：

<img src="/Users/licheng/Documents/Typora/Picture/image-20200715174730811.png" alt="image-20200715174730811" style="zoom:50%;" />

之后将各个节点散列到这个环上，可以用节点的 IP、hostname 这样的唯一性字段作为 Key 进行 `hash(key)`，散列之后如下：

<img src="/Users/licheng/Documents/Typora/Picture/image-20200715174955175.png" alt="image-20200715174955175" style="zoom:50%;" />

之后需要将数据定位到对应的节点上，使用同样的 hash 函数将数据也映射到这个环上。

<img src="/Users/licheng/Documents/Typora/Picture/image-20200715175344002.png" alt="image-20200715175344002" style="zoom:50%;" />

这样按照顺时针方向就可以把 ObjectA 定位到 Node1 节点，ObjectB 定位到 Node2 节点，ObjectC 定位到 Node3 节点。

##### 容错性

假设 Node1 宕机了，依然根据顺时针方向，Node2 和 Node3 保持不变，只有 ObjectA 被重新映射到了 Node2。这样就很好的保证了容错性，当一个节点宕机时只会影响到少量部分的数据。

##### 扩展性

当在 Node2 和 Node3 之间新增了一个节点 Node4 后，这时会发现受影响的数据只有 ObjectC，其余数据也是保持不变，所以这样也很好的保证了拓展性。

##### 虚拟节点

当节点较少时，可能会出现数据分布不均匀的情况。如下图，这样会导致大部分数据在 Node2，只有少部分数据在 Node1。

<img src="/Users/licheng/Documents/Typora/Picture/image-20200715175945362.png" alt="image-20200715175945362" style="zoom:50%;" />

为了解决这个问题，一致哈希算法引入了虚拟节点。将每一个节点都进行多次 hash，生成多个节点放置在环上称为虚拟节点。具体做法可以在服务器 IP 或主机名的后面增加编号来实现。

<img src="/Users/licheng/Documents/Typora/Picture/image-20200715180217381.png" alt="image-20200715180217381" style="zoom:50%;" />

这样只需要在原有的基础上多一步由虚拟节点映射到实际节点的步骤即可让少量节点也能满足均匀性。

#### Redis 集群的高可用模型

为了保证高可用，Redis Cluster 集群引入了主从复制模型，一个主节点对应一个或者多个从节点，当主节点宕机的时候，就会启用从节点。当其它主节点 ping 一个主节点 A 时，如果半数以上的主节点与 A 通信超时，那么认为主节点 A 宕机了。如果主节点和它的从节点 都宕机了，那么该节点就无法再提供服务了。